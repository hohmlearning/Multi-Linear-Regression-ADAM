{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Linear Regression with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'Modules/')\n",
    "from Evaluation_Metric import Metric_regression\n",
    "from Linear_Regression_ADAM import ADAM, ADAM_learning_rate_decay, ADAM_learning_rate_decay_full_train\n",
    "from Cross_validation import preparation_cross_validation\n",
    "golden_section_search = __import__('20220716_Golden_Section_Search').golden_section_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data set\n",
    "The Diabetes dataset is loaded as a toy dataset. \n",
    "The different models are compared based on the diabetes dataset. \n",
    "\n",
    "Additional information are referred to: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset (accessed 18th July 2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - X.mean(axis=0)\n",
    "X = X / X.var(axis=0)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting data in train and test\n",
    "The models are trained based on the training data. The performance evaluation is performed on the test data. \n",
    "Therefore, the data is splitted in train (90 %) and test (10 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(y.shape[0] * 0.9)\n",
    "X_train = X[:n_train,:]\n",
    "y_train = y[:n_train]\n",
    "X_test =  X[n_train:,:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting models to train\n",
    "First, the reference Linear Regression with analytical solution is performed using the modul **Scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Fit direct Linear Regression (Sklearn)\n"
     ]
    }
   ],
   "source": [
    "print('1. Fit direct Linear Regression (Sklearn)')\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_sklearn = np.dot(X_test, lin_reg.coef_) + lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereafter, **Ridge Regression** (Scikit-learn) is deployed to the trainingsset.\n",
    "**Ridge Regression** avoids overfitting with $L^{2}$ regularisation.\n",
    "The trainings dataset is divided into $n$ subtrain and validation sets. \n",
    "Then, the optimal alpha, which minimzes the validation MSE, is found with Cross Validation (CV) for every set.\n",
    "The CV for $n$ datasets is repeated 100 for decreasing the variance of the optimal alphas.\n",
    "The optima are found with the **golden section search**. \n",
    "The **golden section search** finds the mimimum in a given intervall, for strictly unimodal functions.\n",
    "The optimal alpha for the training on the full dataset, is set to the mean of all found alphas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_alpha (alpha, X_train, y_train, X_val, y_val):\n",
    "    ridge_ridge = Ridge(alpha=alpha)\n",
    "    ridge_ridge.fit(X_train, y_train)\n",
    "    y_val_hat = np.dot(X_val, ridge_ridge.coef_) + ridge_ridge.intercept_\n",
    "    MSE_ = Metric_regression().fun_MSE(y_val, y_val_hat)\n",
    "    return (MSE_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_split = int(y.shape[0]*0.55)\n",
    "X_train_1 = X_train[:n_split,:]\n",
    "y_train_1 = y_train[:n_split]\n",
    "X_val_1 = X_train[n_split:,:]\n",
    "y_val_1 = y_train[n_split:]\n",
    "\n",
    "MSE_min = lambda alpha: MSE_alpha(alpha, X_train=X_train_1, y_train=y_train_1, X_val=X_val_1, y_val=y_val_1)\n",
    "MSE_min_vec = np.vectorize(MSE_min)\n",
    "alpha_array = np.linspace(0, 100, 100)\n",
    "plt.plot(alpha_array, MSE_min_vec(alpha_array))\n",
    "plt.xlabel(r'$\\alpha$ / -')\n",
    "plt.ylabel('MSE$_{\\mathrm{val}}$ / -')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE Cross Validation = 3154.19\n",
      "Best alpha = 33.7 +- 38.8\n"
     ]
    }
   ],
   "source": [
    "n_bags = 3\n",
    "n_repetition = 100\n",
    "alpha_opt_list = []\n",
    "MSE_mean = 0\n",
    "for n_rep in range (n_repetition):\n",
    "    bags_list = preparation_cross_validation(X_train, y_train, n_bags)\n",
    "    for n_val in range (n_bags):\n",
    "        X_train_ = []\n",
    "        y_train_ = []\n",
    "        X_val_ = np.array(bags_list[n_val].x)\n",
    "        y_val_ =  np.array(bags_list[n_val].y)\n",
    "        for n in range (n_bags):\n",
    "            if n_val != n:\n",
    "                X_train_ += bags_list[n].x\n",
    "                y_train_ += bags_list[n].y\n",
    "        X_train_ = np.array(X_train_)\n",
    "        y_train_ = np.array(y_train_)\n",
    "\n",
    "        MSE_min = lambda alpha: MSE_alpha(alpha, X_train=X_train_, y_train=y_train_, X_val=X_val_, y_val=y_val_)\n",
    "        MSE_min_vec = np.vectorize(MSE_min)\n",
    "        alpha_opt = golden_section_search(MSE_min, 0, 10000, 1E-8)\n",
    "        alpha_opt_list.append(alpha_opt)\n",
    "        MSE_mean += MSE_min(alpha_opt) / n_bags / n_repetition\n",
    "        #print('MSE (on validation set {}/{}) = {:.2f} | alpha = {:.2g}'.format(n_val+1, n_bags, MSE_min(alpha_opt), alpha_opt))  \n",
    "best_alpha = np.array(alpha_opt_list)\n",
    "print('Mean MSE Cross Validation = {:.2f}'.format(MSE_mean))\n",
    "print('Best alpha = {:.3g} +- {:.3g}'.format(np.mean(best_alpha), best_alpha.var(ddof=1)**0.5))\n",
    "ridge_reg = Ridge(np.mean(best_alpha))\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "y_test_ridge_sklearn = np.dot(X_test, ridge_reg.coef_) + ridge_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the maximum of iteration\n",
    "4 models based on Stochastic Gradient Descent (SGD) with ADAM are fitted.\n",
    "The maximum amount of Epoch is set for all SGD models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 1E5\n",
    "model_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. SGD with Adam on train:\n",
      "Epoch: 10000 | MSE_train: 3020.46\n",
      "Epoch: 20000 | MSE_train: 3012.56\n",
      "Epoch: 30000 | MSE_train: 3005.28\n",
      "Epoch: 40000 | MSE_train: 3010.19\n",
      "Epoch: 50000 | MSE_train: 3007.67\n",
      "Epoch: 60000 | MSE_train: 3010.47\n",
      "Epoch: 70000 | MSE_train: 3016.50\n",
      "Epoch: 80000 | MSE_train: 3010.85\n",
      "Epoch: 90000 | MSE_train: 3006.53\n",
      "Epoch: 100000 | MSE_train: 3006.27\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_1 = ADAM(max_epoch=max_epoch, eta=.1, batch_size=1)\n",
    "print('2. {} on train:'.format(Gradient_descent_1.name))\n",
    "Gradient_descent_1(X_train, y_train) \n",
    "model_list.append(Gradient_descent_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM and learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. SGD with ADAM and learning rate decay on train:\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_2 = ADAM_learning_rate_decay(max_epoch=max_epoch, eta=.1, batch_size=1, patience=1E2)\n",
    "print('3. {} on train:'.format(Gradient_descent_2.name))\n",
    "Gradient_descent_2(X_train, y_train)\n",
    "model_list.append(Gradient_descent_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM and learning rate decay (evaluation on validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. SGD with ADAM and learning rate decay on subtrain:\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_2_2 = ADAM_learning_rate_decay(max_epoch=max_epoch, eta=.1, batch_size=1, patience=1E2)\n",
    "print('4. {} on subtrain:'.format(Gradient_descent_2_2.name))\n",
    "split = int(X_train.shape[0]*0.8)\n",
    "Gradient_descent_2_2(X_train[:split], y_train[:split],\n",
    "                     X_train[split:], y_train[split:]\n",
    "                     )\n",
    "model_list.append(Gradient_descent_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM and learning rate decay (full training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. SGD with ADAM and learning rate decay and full training on subtrain and train, respectively:\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "Start full training!\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_3 = ADAM_learning_rate_decay_full_train(max_epoch=max_epoch, eta=.1, batch_size=1, patience=1E2)\n",
    "print('6. {} on subtrain and train, respectively:'.format(Gradient_descent_3.name))\n",
    "Gradient_descent_3(X_train, y_train)\n",
    "model_list.append(Gradient_descent_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation of the models\n",
    "### Comparing the weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        0        1     2  \\\n",
      "Sklearn                                             152.4 -2.1e-02 -11.3   \n",
      "Rdge                                                152.3  3.6e-01 -10.0   \n",
      "SGD with Adam                                       152.5 -2.0e-01 -12.3   \n",
      "SGD with ADAM and learning rate decay               152.4  3.9e-01 -11.8   \n",
      "SGD with ADAM and learning rate decay metric on...  152.0  9.4e-03 -12.6   \n",
      "SGD with ADAM and learning rate decay and full ...  152.0 -5.7e-02 -12.5   \n",
      "\n",
      "                                                       3     4     5     6  \\\n",
      "Sklearn                                             24.7  14.5 -35.8  20.4   \n",
      "Rdge                                                23.4  13.5  -4.7  -3.0   \n",
      "SGD with Adam                                       24.4  14.8 -63.1  40.5   \n",
      "SGD with ADAM and learning rate decay               24.0  14.7 -53.0  33.7   \n",
      "SGD with ADAM and learning rate decay metric on...  24.6  13.2 -43.9  25.4   \n",
      "SGD with ADAM and learning rate decay and full ...  24.5  13.1 -47.5  28.4   \n",
      "\n",
      "                                                       7     8     9   10  \n",
      "Sklearn                                              4.7  10.2  32.8  4.6  \n",
      "Rdge                                                -8.5   6.5  19.9  5.3  \n",
      "SGD with Adam                                       14.7  13.1  43.9  4.5  \n",
      "SGD with ADAM and learning rate decay               11.7  12.0  40.5  3.9  \n",
      "SGD with ADAM and learning rate decay metric on...   9.3  10.8  40.8  4.5  \n",
      "SGD with ADAM and learning rate decay and full ...  10.8  11.1  42.1  4.5  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "compare_weights = pd.DataFrame()\n",
    "compare_weights['Sklearn'] = np.append(lin_reg.intercept_ , lin_reg.coef_ )\n",
    "compare_weights['Rdge'] = np.append(ridge_reg.intercept_ , ridge_reg.coef_ )\n",
    "for SGD_model in model_list:\n",
    "    column_name = SGD_model.name\n",
    "    if column_name == 'SGD with Adam':\n",
    "        w = SGD_model.theta\n",
    "        w_0 = SGD_model.theta_0\n",
    "    \n",
    "    else:\n",
    "        w = SGD_model.best_theta\n",
    "        w_0 = SGD_model.best_theta_0\n",
    "    compare_weights[column_name] = np.append(w_0, w)\n",
    "pd.set_option('display.precision', 1)\n",
    "print(compare_weights.T)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "The Mean Squared Error (MSE) on the test data set is compared.\n",
    "In addition, the increase in MSE is referenced to the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Deviation / %     MSE\n",
      "Sklearn                                                       0.0  1735.9\n",
      "Ridge                                                        -6.2  1843.9\n",
      "SGD with Adam                                                 2.4  1695.0\n",
      "SGD with ADAM and learning rate decay                         2.6  1690.3\n",
      "SGD with ADAM and learning rate decay metric on...            5.0  1648.8\n",
      "SGD with ADAM and learning rate decay and full ...            5.0  1648.9\n"
     ]
    }
   ],
   "source": [
    "MSE_compare = pd.Series(dtype=float)\n",
    "ARD_compare = pd.Series(dtype=float) \n",
    "MSE_sklearn = Metric_regression().fun_MSE(y_test, y_test_sklearn)\n",
    "MSE_ridge = Metric_regression().fun_MSE(y_test, y_test_ridge_sklearn)\n",
    "MSE_compare['Sklearn'] = MSE_sklearn\n",
    "MSE_compare['Ridge'] = MSE_ridge\n",
    "ARD_compare['Sklearn'] = 0\n",
    "ARD_compare['Ridge'] = -(MSE_ridge - MSE_sklearn)  / MSE_sklearn*100\n",
    "for SGD_model in model_list:\n",
    "    column_name = SGD_model.name\n",
    "    MSE_test = SGD_model.MSE(X_test, y_test)\n",
    "    MSE_compare[column_name] = MSE_test\n",
    "    ARD_compare[column_name] = -(MSE_test-MSE_sklearn) / MSE_sklearn*100\n",
    "\n",
    "MSE_DF = MSE_compare.to_frame(name='MSE')\n",
    "ARD_DF = ARD_compare.to_frame(name='Deviation / %')\n",
    "performance_DF = pd.concat([ARD_DF, MSE_DF], axis=1)\n",
    "print(performance_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical solution of the **Linear Regression** is set to the baseline of the models. \n",
    "SDG with ADAM improves the performance of the test MSE.\n",
    "Allthough SGD with ADAM is adaptive, the performance increases rises from **4.2 %** to **5.0 %** when applying learning rate decay. \n",
    "In case of SGD with ADA; and leraning rate decay, the MSE increase improves fom **2.6 %** to **5.0 %** if the metric is applied on the validation dataset.\n",
    "Therefore, Early Stopping must be evaluated on a validation dataset to avoid overfitting.\n",
    "\n",
    "Interestingly, for this particular dataset the **Ridge Regression** seems not to improve the error on the test set. \n",
    "Probably, the high  standard deviation of the optimized alpha **(33.7 +- 38.8)** causes a non optimal selection of the alpha. A different regulariation, e.g. Lasso or Elastic Net, may improve the performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
