{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Linear Regression with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'Modules/')\n",
    "from Evaluation_Metric import Metric_regression\n",
    "from Linear_Regression_ADAM import ADAM, ADAM_learning_rate_decay, ADAM_learning_rate_decay_full_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data set\n",
    "The Diabetes dataset is loaded as a toy dataset. \n",
    "The different models are compared based on the diabetes dataset. \n",
    "\n",
    "Additional information are referred to: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset (accessed 18th July 2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - X.mean(axis=0)\n",
    "X = X / X.var(axis=0)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting data in train and test\n",
    "The models are trained based on the training data. The performance evaluation is performed on the test data. \n",
    "Therefore, the data is splitted in train (90 %) and test (10 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(y.shape[0] * 0.9)\n",
    "X_train = X[:n_train,:]\n",
    "y_train = y[:n_train]\n",
    "X_test =  X[n_train:,:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting models to train\n",
    "First, the reference Linear Regression with analytical solution is performed using the modul **Scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Fit direct Linear Regression (Sklearn)\n"
     ]
    }
   ],
   "source": [
    "print('1. Fit direct Linear Regression (Sklearn)')\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_sklearn = np.dot(X_test, lin_reg.coef_) + lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the maximum of iteration\n",
    "4 models based on Stochastic Gradient Descent (SGD) with ADAM are fitted.\n",
    "The maximum amount of Epoch is set for all SGD models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 1E4\n",
    "model_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. SGD with Adam on train:\n",
      "Epoch: 1000 | MSE_train: 3005.56\n",
      "Epoch: 2000 | MSE_train: 3020.51\n",
      "Epoch: 3000 | MSE_train: 3015.60\n",
      "Epoch: 4000 | MSE_train: 3007.81\n",
      "Epoch: 5000 | MSE_train: 3008.93\n",
      "Epoch: 6000 | MSE_train: 3008.53\n",
      "Epoch: 7000 | MSE_train: 3009.11\n",
      "Epoch: 8000 | MSE_train: 3008.04\n",
      "Epoch: 9000 | MSE_train: 3011.54\n",
      "Epoch: 10000 | MSE_train: 3020.46\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_1 = ADAM(max_epoch=max_epoch, eta=.1, batch_size=1)\n",
    "print('2. {} on train:'.format(Gradient_descent_1.name))\n",
    "Gradient_descent_1(X_train, y_train) \n",
    "model_list.append(Gradient_descent_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM and learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. SGD with ADAM and learning rate decay on train:\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_2 = ADAM_learning_rate_decay(max_epoch=max_epoch, eta=.1, batch_size=1, patience=1E2)\n",
    "print('3. {} on train:'.format(Gradient_descent_2.name))\n",
    "Gradient_descent_2(X_train, y_train)\n",
    "model_list.append(Gradient_descent_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM and learning rate decay (evaluation on validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. SGD with ADAM and learning rate decay on subtrain:\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_2_2 = ADAM_learning_rate_decay(max_epoch=max_epoch, eta=.1, batch_size=1, patience=1E2)#\n",
    "print('4. {} on subtrain:'.format(Gradient_descent_2_2.name))\n",
    "split = int(X_train.shape[0]*0.8)\n",
    "Gradient_descent_2_2(X_train[:split], y_train[:split],\n",
    "                     X_train[split:], y_train[split:]\n",
    "                     )\n",
    "model_list.append(Gradient_descent_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with ADAM and learning rate decay (full training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. SGD with ADAM and learning rate decay and full training on subtrain and train, respectively:\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "Start full training!\n",
      "New learning rate: 0.032\n",
      "New learning rate: 0.01\n",
      "New learning rate: 0.0032\n",
      "New learning rate: 0.001\n",
      "New learning rate: 0.00032\n",
      "New learning rate: 0.0001\n",
      "New learning rate: 3.2e-05\n",
      "New learning rate: 1e-05\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "Gradient_descent_3 = ADAM_learning_rate_decay_full_train(max_epoch=max_epoch, eta=.1, batch_size=1, patience=1E2)\n",
    "print('6. {} on subtrain and train, respectively:'.format(Gradient_descent_3.name))\n",
    "Gradient_descent_3(X_train, y_train)\n",
    "model_list.append(Gradient_descent_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation of the models\n",
    "### Comparing the weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        0        1     2  \\\n",
      "Sklearn                                             152.4 -2.1e-02 -11.3   \n",
      "SGD with Adam                                       151.8  8.6e-01 -12.2   \n",
      "SGD with ADAM and learning rate decay               152.0  9.4e-03 -12.6   \n",
      "SGD with ADAM and learning rate decay and full ...  152.0 -5.7e-02 -12.5   \n",
      "\n",
      "                                                       3     4     5     6  \\\n",
      "Sklearn                                             24.7  14.5 -35.8  20.4   \n",
      "SGD with Adam                                       23.6  15.3 -61.0  40.4   \n",
      "SGD with ADAM and learning rate decay               24.6  13.2 -43.9  25.4   \n",
      "SGD with ADAM and learning rate decay and full ...  24.5  13.1 -47.5  28.4   \n",
      "\n",
      "                                                       7     8     9   10  \n",
      "Sklearn                                              4.7  10.2  32.8  4.6  \n",
      "SGD with Adam                                       16.5  13.2  44.1  3.2  \n",
      "SGD with ADAM and learning rate decay                9.3  10.8  40.8  4.5  \n",
      "SGD with ADAM and learning rate decay and full ...  10.8  11.1  42.1  4.5  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "compare_weights = pd.DataFrame()\n",
    "compare_weights['Sklearn'] = np.append(lin_reg.intercept_ , lin_reg.coef_ )\n",
    "\n",
    "for SGD_model in model_list:\n",
    "    column_name = SGD_model.name\n",
    "    if column_name == 'SGD with Adam':\n",
    "        w = SGD_model.theta\n",
    "        w_0 = SGD_model.theta_0\n",
    "    \n",
    "    else:\n",
    "        w = SGD_model.best_theta\n",
    "        w_0 = SGD_model.best_theta_0\n",
    "    compare_weights[column_name] = np.append(w_0, w)\n",
    "pd.set_option('display.precision', 1)\n",
    "print(compare_weights.T)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "The Mean Squared Error (MSE) on the test data set is compared.\n",
    "In addition, the increase in MSE is referenced to the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Deviation / %     MSE\n",
      "Sklearn                                                       0.0  1735.9\n",
      "SGD with Adam                                                 4.2  1662.6\n",
      "SGD with ADAM and learning rate decay                         5.0  1648.8\n",
      "SGD with ADAM and learning rate decay and full ...            5.0  1648.9\n"
     ]
    }
   ],
   "source": [
    "MSE_compare = pd.Series(dtype=float)\n",
    "ARD_compare = pd.Series(dtype=float) \n",
    "MSE_sklearn = Metric_regression().fun_MSE(y_test, y_test_sklearn)\n",
    "MSE_compare['Sklearn'] = MSE_sklearn\n",
    "ARD_compare['Sklearn'] = 0\n",
    "for SGD_model in model_list:\n",
    "    column_name = SGD_model.name\n",
    "    MSE_test = SGD_model.MSE(X_test, y_test)\n",
    "    MSE_compare[column_name] = MSE_test\n",
    "    ARD_compare[column_name] = -(MSE_test-MSE_sklearn)/MSE_sklearn*100\n",
    "\n",
    "MSE_DF = MSE_compare.to_frame(name='MSE')\n",
    "ARD_DF = ARD_compare.to_frame(name='Deviation / %')\n",
    "performance_DF = pd.concat([ARD_DF, MSE_DF], axis=1)\n",
    "print(performance_DF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
